AutoSAGE

AI platform that transforms raw data into diagnosis, modeling, explainability, and actionable recommendations â€” fully automated, integrated, and production-ready.

ğŸš€ Overview

AutoSAGE automates the full pipeline:

connect â†’ ingest â†’ diagnose â†’ audit â†’ explore â†’ model â†’ explain â†’ recommend â†’ expose via API

Created for companies that need clarity, speed, and data-driven decision-making â€” with or without a specialized team.

ğŸ’° Investor Pitch

(kept â€” only enriched to match architectural evolution)

AutoSAGE exists because 80% of data science work is wasted cleaning, diagnosing, and explaining data â€” not modeling.

No leading platform solves this gap with:

transparency

explainability

auditability

end-to-end autonomy

With its new modular architecture (DCP â†’ EDA â†’ ML â†’ ORC), AutoSAGE has evolved into a scientific automation platform, not just AutoML.

âœ¨ Key Features
ğŸ”Œ Connectivity & Ingestion

Native connector for Postgres

Direct reading of any table (schema.table)

Automatic schema and type detection

Secure loading via secrets

Support for DataFrame, CSV, and SQL (roadmap)

New (v2025): DCP architecture with fully automated ingestion

ğŸ“¥ Intelligent Ingestion

Column standardization

Automatic target detection

Robust date and encoding conversion

Initial schema validation

Pipeline orchestrated through the DCP â†’ EDA modules

ğŸ©º Data Diagnosis & Quality

Missing values

Outliers (Z-score, IQR, robust stats)

Cardinality and structure

Structural drift

Descriptive statistics and distributions

ğŸ”¬ Auto-EDA

Correlations (Pearson, Spearman, CramÃ©râ€™s V)

Hypothesis tests (t-test, ANOVA, Ï‡Â²)

Pre-modeling insights

Detection of weak variables

Automatic visualizations

Export is now 100% PARQUET (new official standard)

ğŸ¤– Automatic Model Selection

Classification: Logistic, SVM, Random Forest, Gradient Boosting

Regression: Linear, Ridge, Random Forest, XGBoost

Selection based on biasâ€“variance, stability, and interpretability

Integrated with a new ML module fully isolated and versioned

ğŸ‹ï¸ Training

Stratified train/test split

Automatic normalization and encoding

Cross-validation

Simple hyperparameter search

Reproducible pipeline

Current architecture executes the entire training inside the ML container autonomously

ğŸ“Š Metrics & Comparisons

Classification â†’ AUC, F1, Precision, Recall

Regression â†’ RMSE, MAE, RÂ², MAPE

Mandatory baseline comparison

Automatically generated full HTML reports

ğŸ” Explainability

Feature importance

SHAP values

Model behavior analysis

Bias detection

ğŸ“¦ Export & Registry

Automatic saving of the best model (/models/)

Exported artifacts:

Model

Metrics

Feature importance

Logs

PARQUET files

Internal versioning via execution hash

Unified registry shared between modules via Docker volumes

ğŸ“¡ Inference API (implemented)

FastAPI in src/app/main.py

/predict endpoint

Automatic validation with Pydantic

Model loading from registry

Response includes prediction + explainability

Structured logging per request

ğŸ“ˆ Monitoring & Logs

Logs persisted under /logs/

Execution IDs

Drift warnings

Full pipeline audit trail

Trace ID propagated across all modules (ORC â†’ DCP â†’ EDA â†’ ML)

ğŸ†• ğŸ”§ DCP Connector Module

The DCP (Data Connector Pipeline) is the new AutoSAGE layer that connects to external databases and ingests tables automatically, without relying on manual uploads.

What has been implemented

Fully functional Postgres connector

Direct ingestion of the customer_churn table from the dcp database

Secure credential loading via Secrets Manager

Internal configuration registry

Structured and standardized logs

/ingest endpoint to trigger data collection

Automatic pipeline DCP â†’ EDA â†’ ML

Manual upload removed by strategic decision

Ready for expansion

MySQL

SQL Server

BigQuery

S3

External REST APIs

Philosophy

Plug-and-play connectors

Orchestrated and safe execution

Architecture designed for enterprise environments

ğŸ†• Modular Architecture 2025

ORC (Orchestrator): controls and guarantees the entire data flow

DCP: collects and standardizes

EDA: diagnoses, audits, and prepares artifacts

ML: models, evaluates, and generates reports

All connected by versioned registry + distributed trace ID

ğŸ”¬ Scientific Methodology

(kept exactly as original â€” only updated where necessary)

1ï¸âƒ£ Ingestion & Standardization

Automatic typing

Column normalization

Date conversion and validation

Standardized pipeline inside the DCP module

2ï¸âƒ£ Statistical Diagnosis

Distributions and densities

Descriptive statistics

Cardinality

Artifacts now exported as PARQUET

3ï¸âƒ£ Quality Audit

Missing values

Outliers

Semantic inconsistencies

Structural drift

4ï¸âƒ£ Relationships & Statistical Signal

Correlations

Hypothesis tests

Preliminary feature importance

5ï¸âƒ£ Intelligent Model Selection

Based on target and variable structure

6ï¸âƒ£ Reproducible Training

Stratified splits

Automatic encoding and scaling

Cross-validation

Execution isolated inside the ML module

7ï¸âƒ£ Transparent Metrics

Complete classification and regression metrics

8ï¸âƒ£ Explainability

SHAP

Feature importance

Bias detection

9ï¸âƒ£ Actionable Recommendations

Suggested next steps

Recommended interventions

Risks and limitations

âš”ï¸ Strategic Comparison

(kept â€” unchanged structurally)

ğŸ¯ Target Market

(unchanged)

ğŸ’µ Monetization

(unchanged)

ğŸ§  Strategic Advantages

(unchanged â€” now strengthened by the new modular core)

ğŸŒ Vision

If data exists, clarity should exist.
And clarity should be automatic.

We are building the universal interpretation layer between data and decision â€” now with a distributed, scalable, production-ready architecture.

ğŸ“Š Documentation

(unchanged)

ğŸ›¡ï¸ License

MIT

ğŸ’¡ Contact

ğŸ”— LinkedIn â€” https://www.linkedin.com/in/sergiofonsecasilva

ğŸ“© sergiofs.u1tec@gmail.com

ğŸ“ +55 11 9 3767-8996